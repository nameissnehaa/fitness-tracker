{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #type: ignore\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        # TODO: return output\n",
    "        pass\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        # TODO: update parameters and return input gradient\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(output_size, input_size)\n",
    "        self.bias = np.random.randn(output_size, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(self.weights, self.input) + self.bias\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "       input_gradient = np.dot(self.weights.T, output_gradient)\n",
    "       bias_gradient = np.sum(output_gradient, axis=1, keepdims=True)\n",
    "       self.weights -= learning_rate * np.dot(output_gradient, self.input.T)\n",
    "       self.bias -= learning_rate * bias_gradient\n",
    "       return input_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self, activation, activation_derivative):\n",
    "        self.activation = activation\n",
    "        self.activation_derivative = activation_derivative\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(self.input)\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        #YOUR CODE HERE\n",
    "        activation_grad = self.activation_derivative(self.input)\n",
    "        input_gradient = output_gradient * activation_grad\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "\n",
    "        def sigmoid_derivative(x):\n",
    "            s = sigmoid(x)\n",
    "            return s * (1 - s)\n",
    "        \n",
    "        super().__init__(sigmoid, sigmoid_derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    return np.mean(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_cross_entropy_derivative(y_true, y_pred):\n",
    "    return ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)\n",
    "network = [\n",
    "    Dense(2, 3),\n",
    "    Sigmoid(),\n",
    "    Dense(3, 1),\n",
    "    Sigmoid()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(network, input):\n",
    "    \n",
    "    for layer in network:\n",
    "        output = layer.forward(input)\n",
    "        # print(f\"\\n  {layer.__class__.__name__} Output  \\n {output}\")\n",
    "        input = output\n",
    "        \n",
    "    return output\n",
    "# Input data (features) and true label\n",
    "X = np.array([\n",
    "    [0.5, 1.5],\n",
    "    [1.0, 2.0],\n",
    "    [1.5, 0.5],\n",
    "    [3.0, 1.0]\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([[0], [0], [1], [1]])  # True output labels\n",
    "def train(network, loss, loss_derivative, x_train, y_train, epochs = 100, learning_rate = 0.01, verbose = True):\n",
    "    for e in range(epochs):\n",
    "        error = 0\n",
    "        for i in range(len(x_train)):\n",
    "            inputs = x_train[i].reshape(-1,1) \n",
    "            target = y_train[i].reshape(-1,1)\n",
    "            output = inputs\n",
    "            for layer in network:\n",
    "                output = layer.forward(output)\n",
    "            error+=loss(target,output)\n",
    "            grad = loss_derivative(target, output)\n",
    "            for layer in reversed(network):  # Backpropagation through each layer\n",
    "                grad = layer.backward(grad, learning_rate)\n",
    "        error /= len(x_train)\n",
    "        if verbose:\n",
    "            print(f\"{e + 1}/{epochs}, error={round(error, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100, error=0.26\n",
      "2/100, error=0.25\n",
      "3/100, error=0.25\n",
      "4/100, error=0.24\n",
      "5/100, error=0.24\n",
      "6/100, error=0.23\n",
      "7/100, error=0.23\n",
      "8/100, error=0.23\n",
      "9/100, error=0.22\n",
      "10/100, error=0.22\n",
      "11/100, error=0.22\n",
      "12/100, error=0.21\n",
      "13/100, error=0.21\n",
      "14/100, error=0.21\n",
      "15/100, error=0.2\n",
      "16/100, error=0.2\n",
      "17/100, error=0.2\n",
      "18/100, error=0.19\n",
      "19/100, error=0.19\n",
      "20/100, error=0.19\n",
      "21/100, error=0.19\n",
      "22/100, error=0.18\n",
      "23/100, error=0.18\n",
      "24/100, error=0.18\n",
      "25/100, error=0.18\n",
      "26/100, error=0.17\n",
      "27/100, error=0.17\n",
      "28/100, error=0.17\n",
      "29/100, error=0.17\n",
      "30/100, error=0.16\n",
      "31/100, error=0.16\n",
      "32/100, error=0.16\n",
      "33/100, error=0.16\n",
      "34/100, error=0.16\n",
      "35/100, error=0.15\n",
      "36/100, error=0.15\n",
      "37/100, error=0.15\n",
      "38/100, error=0.15\n",
      "39/100, error=0.15\n",
      "40/100, error=0.14\n",
      "41/100, error=0.14\n",
      "42/100, error=0.14\n",
      "43/100, error=0.14\n",
      "44/100, error=0.14\n",
      "45/100, error=0.14\n",
      "46/100, error=0.13\n",
      "47/100, error=0.13\n",
      "48/100, error=0.13\n",
      "49/100, error=0.13\n",
      "50/100, error=0.13\n",
      "51/100, error=0.13\n",
      "52/100, error=0.12\n",
      "53/100, error=0.12\n",
      "54/100, error=0.12\n",
      "55/100, error=0.12\n",
      "56/100, error=0.12\n",
      "57/100, error=0.12\n",
      "58/100, error=0.12\n",
      "59/100, error=0.12\n",
      "60/100, error=0.11\n",
      "61/100, error=0.11\n",
      "62/100, error=0.11\n",
      "63/100, error=0.11\n",
      "64/100, error=0.11\n",
      "65/100, error=0.11\n",
      "66/100, error=0.11\n",
      "67/100, error=0.11\n",
      "68/100, error=0.11\n",
      "69/100, error=0.1\n",
      "70/100, error=0.1\n",
      "71/100, error=0.1\n",
      "72/100, error=0.1\n",
      "73/100, error=0.1\n",
      "74/100, error=0.1\n",
      "75/100, error=0.1\n",
      "76/100, error=0.1\n",
      "77/100, error=0.1\n",
      "78/100, error=0.1\n",
      "79/100, error=0.09\n",
      "80/100, error=0.09\n",
      "81/100, error=0.09\n",
      "82/100, error=0.09\n",
      "83/100, error=0.09\n",
      "84/100, error=0.09\n",
      "85/100, error=0.09\n",
      "86/100, error=0.09\n",
      "87/100, error=0.09\n",
      "88/100, error=0.09\n",
      "89/100, error=0.09\n",
      "90/100, error=0.09\n",
      "91/100, error=0.09\n",
      "92/100, error=0.08\n",
      "93/100, error=0.08\n",
      "94/100, error=0.08\n",
      "95/100, error=0.08\n",
      "96/100, error=0.08\n",
      "97/100, error=0.08\n",
      "98/100, error=0.08\n",
      "99/100, error=0.08\n",
      "100/100, error=0.08\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    network,\n",
    "    binary_cross_entropy,\n",
    "    binary_cross_entropy_derivative,\n",
    "    X,\n",
    "    y_true,\n",
    "    epochs=100,\n",
    "    learning_rate=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
